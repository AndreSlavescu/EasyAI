{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreSlavescu/EasyAI/blob/main/MLSystemsGroup_Lecture1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 1: Introduction to PyTorch and Graph Compilation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basics of Model Definitions with 'nn.Module'\n",
        "\n",
        "### Model Forward Pass\n",
        "\n",
        "In PyTorch, defining models is very simple. You can create a model by subclassing `nn.Module` and defining the layers and forward pass.\n",
        "\n",
        "### Model Backward Pass, Powered by Autodiff Semantics\n",
        "\n",
        "PyTorch uses a technique called automatic differentiation (autodiff) to automatically compute gradients. This part of model definition is abstracted away from the developer, however this is a necessary component for training models as will be seen in future lectures.\n",
        " \n",
        "### How Autodiff Works\n",
        "\n",
        "Under the hood, PyTorch builds a dynamic computational graph as operations are performed on tensors. Each node in this graph represents a tensor, and edges represent the operations that produce the output tensors from input tensors.\n",
        "\n",
        "### Explicit Backward Calls\n",
        "\n",
        "When you call `.backward()` on a tensor, PyTorch traverses the previously mentioned dynamic computational graph from the output tensor back to the input tensors, computing gradients along the way. These gradients are then stored in the `.grad` attribute of the tensors, which can be used to update the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXooFKTB4xl7",
        "outputId": "29daa03f-7d2b-4743-eca8-1a2041b306ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.5874, -0.0648]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Simple Definition of a Neural Network\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear1 = nn.Linear(10, 5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(5, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "x = torch.randn(1, 10)\n",
        "model = SimpleNet()\n",
        "output = model(x)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Representation of the Compute Graph\n",
        "\n",
        "### The 'fx.Graph'\n",
        "\n",
        "The `fx.Graph` in PyTorch is a tool that helps you see and understand the sequence of operations in your model. It creates a visual representation of the model's computation steps, making it easier to debug and optimize.\n",
        "\n",
        "### Symbolic Tracing with `torch.fx`\n",
        "\n",
        "PyTorch provides a module called `torch.fx` that allows for symbolic tracing of the computation graph. Symbolic tracing captures the operations performed on tensors and represents them in a graph structure. This can be useful for debugging, optimization, and understanding the flow of data through the model.\n",
        "\n",
        "`torch.fx` works by recording the operations as they are executed and creating a graph representation of these operations. This graph can then be analyzed, transformed, and optimized. The `symbolic_trace` function is used to perform the tracing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4k8BYdK5q4L",
        "outputId": "144c31a3-e16d-471b-940b-b0eb9ac34440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time with no_grad: 5.3e-05 seconds\n",
            "Time with grad: 0.000113 seconds\n",
            "\n",
            "Graph with torch.no_grad:\n",
            "graph():\n",
            "    %x : [num_users=1] = placeholder[target=x]\n",
            "    %linear1 : [num_users=1] = call_module[target=linear1](args = (%x,), kwargs = {})\n",
            "    %relu : [num_users=1] = call_module[target=relu](args = (%linear1,), kwargs = {})\n",
            "    %linear2 : [num_users=1] = call_module[target=linear2](args = (%relu,), kwargs = {})\n",
            "    return linear2\n",
            "\n",
            "Graph without torch.no_grad:\n",
            "graph():\n",
            "    %x : [num_users=1] = placeholder[target=x]\n",
            "    %linear1 : [num_users=1] = call_module[target=linear1](args = (%x,), kwargs = {})\n",
            "    %relu : [num_users=1] = call_module[target=relu](args = (%linear1,), kwargs = {})\n",
            "    %linear2 : [num_users=1] = call_module[target=linear2](args = (%relu,), kwargs = {})\n",
            "    return linear2\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Comparing the FX Graphs with and without torch.no_grad\n",
        "\n",
        "Analysis:\n",
        "  The `torch.no_grad` context manager is used to disable gradient calculation, \n",
        "  which can significantly speed up inference. This is particularly useful when\n",
        "  you are only performing forward passes through the network and do not need to\n",
        "  compute gradients or perform backpropagation. \n",
        "  \n",
        "  It is important to note, when using torch.no_grad, the fx.Graph representation\n",
        "  doesn't actually change!\n",
        "\"\"\"\n",
        "\n",
        "from torch.fx import symbolic_trace\n",
        "import time\n",
        "\n",
        "time_average_no_grad = 0\n",
        "time_average_with_grad = 0\n",
        "iters = 100\n",
        "\n",
        "for _ in range(iters):\n",
        "  with torch.no_grad():\n",
        "      x = torch.randn(1, 10)\n",
        "      start_no_grad = time.time()\n",
        "      output_no_grad = model(x)\n",
        "      end_no_grad = time.time()\n",
        "      time_average_no_grad += end_no_grad - start_no_grad\n",
        "\n",
        "  x = torch.randn(1, 10)\n",
        "  start_with_grad = time.time()\n",
        "  output_with_grad = model(x)\n",
        "  end_with_grad = time.time()\n",
        "  time_average_with_grad += end_with_grad - start_with_grad\n",
        "\n",
        "print(f'Time with no_grad: {round(time_average_no_grad / iters, 6)} seconds')\n",
        "print(f'Time with grad: {round(time_average_with_grad / iters, 6)} seconds')\n",
        "\n",
        "with torch.no_grad():\n",
        "    traced_model_no_grad = symbolic_trace(model)\n",
        "\n",
        "traced_model_with_grad = symbolic_trace(model)\n",
        "\n",
        "print(\"\\nGraph with torch.no_grad:\")\n",
        "print(traced_model_no_grad.graph)\n",
        "\n",
        "print(\"\\nGraph without torch.no_grad:\")\n",
        "print(traced_model_with_grad.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Profiling\n",
        "\n",
        "## Torch Profiler\n",
        "\n",
        "The `torch.profiler` module enables you to collect detailed information about the execution of your model, including CPU and GPU activities, memory usage, and operator-level statistics. This information can insight into many areas of improvement for a given model you create.\n",
        "\n",
        "## Trace\n",
        "\n",
        "To view your model's trace in a very detailed manner, make sure to use the chrome trace tool. By doing this, you will see exactly how the operators are dispatched over time, and where the more granular bottlenecks are.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1AcTnhzqYmHN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Looking at dispatched operators with trace.json\n",
        "\"\"\"\n",
        "\n",
        "import torch.profiler\n",
        "\n",
        "with torch.no_grad():\n",
        "    with torch.profiler.profile(\n",
        "        activities=[torch.profiler.ProfilerActivity.CPU],\n",
        "        record_shapes=True,\n",
        "        profile_memory=True\n",
        "    ) as prof_no_grad:\n",
        "        output_no_grad = model(x)\n",
        "\n",
        "prof_no_grad.export_chrome_trace(\"trace_no_grad.json\")\n",
        "\n",
        "with torch.profiler.profile(\n",
        "    activities=[torch.profiler.ProfilerActivity.CPU],\n",
        "        record_shapes=True,\n",
        "        profile_memory=True\n",
        ") as prof_with_grad:\n",
        "    output_with_grad = model(x)\n",
        "\n",
        "prof_with_grad.export_chrome_trace(\"trace_with_grad.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fItjOr-XbPVb"
      },
      "source": [
        "# View Trace\n",
        "\n",
        "Visit:\n",
        "\n",
        "[chrome://tracing/](chrome://tracing)\n",
        "\n",
        "to view the trace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO9DhCP4BRTTAcc/zVckq7p",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
